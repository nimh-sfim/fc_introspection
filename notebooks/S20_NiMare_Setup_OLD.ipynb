{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fe374d-7e39-475a-9825-0b46c3ae0cc1",
   "metadata": {},
   "source": [
    "# PART 1 - DataBase Preparation\n",
    "\n",
    "This will download the necessary files and convert them into two local datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90cd90d-1b12-4f9d-888d-949849e59928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing nilearn masking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nilearn/__init__.py:69: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully imported nilearn masking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nimare/__init__.py:74: FutureWarning: Python 3.6 and 3.7 support is deprecated and will be removed in release 0.1.0 of NiMARE. Consider switching to Python 3.8, 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "print(\"importing nilearn masking\")\n",
    "from nilearn import masking\n",
    "print(\"successfully imported nilearn masking\")\n",
    "from nimare import dataset, meta\n",
    "from nimare.extract import fetch_neurosynth\n",
    "from nimare.io import convert_neurosynth_to_dataset\n",
    "from nimare.stats import pearson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ceb7cc-8f79-4036-8aa1-6571f7df44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRJDIR = \"/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/\"\n",
    "vocab = 'LDA50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03c185e-3943-4785-94c6-1160d3a16940",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_DIR  = osp.join(PRJDIR,vocab)\n",
    "NIMARE_PATH   = osp.join(RESOURCE_DIR,'nimare')\n",
    "METAMAPS_DIR  = os.path.join(RESOURCE_DIR,\"meta-analyses-orig\")  # where to save meta-analysis maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa815f-6e5a-4e25-8d15-b745f9af8002",
   "metadata": {},
   "source": [
    "## 1.1. Create output folders\n",
    "> **NOTE:** will delete if they already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260f940-b3fc-48e1-8663-f863532f4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty Output Folders\n",
    "# ===========================\n",
    "print(\"++ INFO: Setting up all necessary folders\")\n",
    "for folder_path in [RESOURCE_DIR, NIMARE_PATH, METAMAPS_DIR]:\n",
    "    if osp.exists(folder_path):\n",
    "        print(\" + WARNING: Removing folder [%s]\" % folder_path)\n",
    "        shutil.rmtree(folder_path)\n",
    "    print(\" + INFO: Generating/Regenerating output folder [%s]\" % folder_path)\n",
    "    os.mkdir(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d8627-cca0-42ac-a423-3a73a420beee",
   "metadata": {},
   "source": [
    "## 1.2. Download Neurosynth database for the selected vocabulary\n",
    "\n",
    "This will take approximately 5 mins, but does not have big memory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa1bb18-5919-447c-a849-2c20745358d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Fetching neurosynth dataset for this vocabulary...\n"
     ]
    }
   ],
   "source": [
    "print(\"++ INFO: Fetching neurosynth dataset for this vocabulary...\")\n",
    "dset_file    = os.path.join(RESOURCE_DIR, \"neurosynth_dataset.pkl.gz\")\n",
    "dset_ma_file = os.path.join(RESOURCE_DIR, \"neurosynth_dataset_with_ma.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a83841-2136-4038-9c2c-d7701752dee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.extract.utils:Dataset found in /data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400/neurosynth\n",
      "\n",
      "INFO:nimare.extract.extract:Searching for any feature files matching the following criteria: [('vocab-LDA400', 'data-neurosynth', 'version-7')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data-neurosynth_version-7_coordinates.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_metadata.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n"
     ]
    }
   ],
   "source": [
    "files = fetch_neurosynth(\n",
    "        data_dir=RESOURCE_DIR,\n",
    "        version=\"7\",\n",
    "        overwrite=False,\n",
    "        vocab=vocab)\n",
    "#,\n",
    "#        source=\"abstract\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049a992d-b7a9-4ca7-ba44-bef3d634f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurosynth_db = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d04e595-0050-44b3-9f65-cd0217e3069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nimare.utils:Not applying transforms to coordinates in unrecognized space 'UNKNOWN'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 2.65 s, total: 3min 3s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neurosynth_dset = convert_neurosynth_to_dataset(\n",
    "        coordinates_file=neurosynth_db[\"coordinates\"],\n",
    "        metadata_file=neurosynth_db[\"metadata\"],\n",
    "        annotations_files=neurosynth_db[\"features\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6438ac-6a11-4779-85b1-4f540315e29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + Saving dataset to /data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400/neurosynth_dataset.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset as a pickle file to the Resources directory\n",
    "print (\" + Saving dataset to %s\" % dset_file)\n",
    "neurosynth_dset.save(dset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd8ef79-00d6-4d76-9342-67860ead8485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + Neurosynth dataset: Dataset(14371 experiments, space='mni152_2mm')\n"
     ]
    }
   ],
   "source": [
    "print(\" + Neurosynth dataset: %s\" % neurosynth_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52a2be-a4cf-4b22-b14d-094d9dd7ec8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01927e31-a53a-4eb9-8c97-29550b01f3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63659174-0f5e-42fe-999a-e687dccaa535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab67ba7-927a-4ab4-afae-6c642dcbda78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4eae0-67be-432e-a0b9-537d0e65469b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1744a1-ac3f-44ff-92e2-510b0aa72e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef218ae1-e1bd-4bda-9ba9-929602f4d6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d72933-5f3f-4d74-ae44-8c33c12d48d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d7035-dc30-45ec-ac8a-6d7e3aa0eb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7457c-c456-4534-9c28-db43f6701d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0643d29a-aa3f-4355-afb7-16d75e16d458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurosynth_dset.get_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f7beea-7434-4b59-a376-c2beb850a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.extract.utils:Dataset found in /data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth\n",
      "\n",
      "INFO:nimare.extract.extract:Searching for any feature files matching the following criteria: [('source-abstract', 'vocab-LDA400', 'data-neurosynth', 'version-7')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data-neurosynth_version-7_coordinates.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_metadata.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_keys.tsv\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_metadata.json\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_source-abstract_type-weight_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-LDA400_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "=====================\n",
      "[{'coordinates': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_coordinates.tsv.gz', 'metadata': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_metadata.tsv.gz', 'features': [{'features': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_source-abstract_type-weight_features.npz', 'vocabulary': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_vocabulary.txt', 'keys': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_keys.tsv', 'metadata': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_metadata.json'}]}]\n",
      "=====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nimare/io.py\u001b[0m in \u001b[0;36mconvert_neurosynth_to_dataset\u001b[0;34m(coordinates_file, metadata_file, annotations_files, feature_groups, target)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mmetadata_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mannotations_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mfeature_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nimare/io.py\u001b[0m in \u001b[0;36mconvert_neurosynth_to_dict\u001b[0;34m(coordinates_file, metadata_file, annotations_files, feature_groups)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_metadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mcoord_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mstudy_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mstudy_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.isfile(dset_file):\n",
    "    files = fetch_neurosynth(\n",
    "        data_dir=RESOURCE_DIR,\n",
    "        version=\"7\",\n",
    "        overwrite=False,\n",
    "        source=\"abstract\",\n",
    "        vocab=vocab)\n",
    "    print('=====================')\n",
    "    print(files)\n",
    "    print('=====================')\n",
    "    neurosynth_db = files[0]\n",
    "\n",
    "    neurosynth_dset = convert_neurosynth_to_dataset(\n",
    "        coordinates_file=neurosynth_db[\"coordinates\"],\n",
    "        metadata_file=neurosynth_db[\"metadata\"],\n",
    "        annotations_files=neurosynth_db[\"features\"],\n",
    "        )\n",
    "    # Save the dataset as a pickle file to the Resources directory\n",
    "    print (\" + Saving dataset to %s\" % dset_file)\n",
    "    neurosynth_dset.save(dset_file)\n",
    "else:\n",
    "    print (\" + Loading dataset from %s\" % dset_file)\n",
    "    neurosynth_dset = dataset.Dataset.load(dset_file)\n",
    "    \n",
    "print(\" + Neurosynth dataset: %s\" % neurosynth_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba2516c-c155-4d82-8726-5f5076fa0519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coordinates': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_coordinates.tsv.gz',\n",
       " 'metadata': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_metadata.tsv.gz',\n",
       " 'features': [{'features': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_source-abstract_type-weight_features.npz',\n",
       "   'vocabulary': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_vocabulary.txt',\n",
       "   'keys': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_keys.tsv',\n",
       "   'metadata': '/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/LDA400.interactive/neurosynth/data-neurosynth_version-7_vocab-LDA400_metadata.json'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1785a-201b-4ca8-bca1-2f48259eac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Neurosynth dataset path (for meta-analysis intermediate outputs)\n",
    "print(\"++ INFO: Updating Neurosynth Object path to [%s]\" % NIMARE_PATH)\n",
    "neurosynth_dset.update_path(NIMARE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9310f6-a0aa-41f4-afaa-a3e9f4ef5383",
   "metadata": {},
   "source": [
    "## 1.3. Convert Neurosynth Database to MA format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06a304-e01c-4fd1-9b1d-20bd9e10dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Estimator\n",
    "print(\"++ INFO: Initializing estimator\")\n",
    "# You could use `memory_limit` here if you want, but that will slow things down.\n",
    "# Niamre version 0.0.9 will take the memory_limit input, version 0.0.13 will not\n",
    "meta_estimator = meta.cbma.mkda.MKDAChi2(memory_limit=ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd2e89-87df-4fcc-ac31-63038d30d7aa",
   "metadata": {},
   "source": [
    "> **NOTE:** This step will require about 100 GB of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9ab2a-b7f0-4b5d-a788-ffad9b52e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-generate MA maps to speed the meta-analyses up. This step may take some time.\n",
    "# This step will create tons of temporary files in NIMARE_PATH\n",
    "# Independently of memory_limit, it looks like this steps makes mem usage to go up to approx 100GB\n",
    "# This software runs with NiMare 0.0.9... it breaks with 0.0.13 (current as of March 2023)\n",
    "print(\"++ INFO: Pre-generate MA maps to speed the meta-analyses up\")\n",
    "kernel_transformer = meta_estimator.kernel_transformer\n",
    "neurosynth_dset = kernel_transformer.transform(neurosynth_dset, return_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb5abb-ec31-46c0-af8b-04ee0f0970fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neurosynth object updated with MA estimations\n",
    "print(\"++ INFO: Saving updated NeuroSynth object [%s]\" % dset_ma_file)\n",
    "neurosynth_dset.save(dset_ma_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c565e-5c36-4fb8-b2e8-0afafbdaf725",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Generate Meta-Analytic Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9b900c-ad8d-466c-8ef3-34f1c8ae3d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing nilearn masking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nilearn/__init__.py:69: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully imported nilearn masking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nimare/__init__.py:74: FutureWarning: Python 3.6 and 3.7 support is deprecated and will be removed in release 0.1.0 of NiMARE. Consider switching to Python 3.8, 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "print(\"importing nilearn masking\")\n",
    "from nilearn import masking\n",
    "print(\"successfully imported nilearn masking\")\n",
    "from nimare import dataset, meta\n",
    "from nimare.extract import fetch_neurosynth\n",
    "from nimare.io import convert_neurosynth_to_dataset\n",
    "from nimare.stats import pearson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1659c2cb-1237-466c-9a25-f6f464893cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRJDIR = \"/data/SFIMJGC_Introspec/prj2021_dyneusr/Resources_NiMare/\"\n",
    "vocab = 'LDA400'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559e74c9-be65-41a5-9e15-526981234fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_DIR  = osp.join(PRJDIR,vocab+\".interactive\")\n",
    "NIMARE_PATH   = osp.join(RESOURCE_DIR,'nimare')\n",
    "METAMAPS_DIR  = os.path.join(RESOURCE_DIR,\"meta-analyses-orig\")  # where to save meta-analysis maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635159c7-27f1-4ae5-a596-8d8c24159f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Fetching neurosynth dataset for this vocabulary...\n"
     ]
    }
   ],
   "source": [
    "print(\"++ INFO: Fetching neurosynth dataset for this vocabulary...\")\n",
    "dset_file    = os.path.join(RESOURCE_DIR, \"neurosynth_dataset.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61976cf6-0616-479c-a152-f58f642d2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurosynth_dset = dataset.Dataset.load(dset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a860263-9fd4-4b4c-a7ee-7d2009a8d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = neurosynth_dset.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4460e9ce-ee5a-4982-9949-fa80a6776ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Project Dir:                  /data/SFIMJGC_Introspec/2023_fc_introspection\n",
      "++ INFO: Bash Scripts Dir:             /data/SFIMJGC_Introspec/2023_fc_introspection/code/fc_introspection/bash\n",
      "++ INFO: Data Dir:                     /data/SFIMJGC_Introspec/pdn\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import os\n",
    "from datetime import datetime\n",
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "from utils.basics import get_sbj_scan_list\n",
    "\n",
    "from utils.basics import PRJ_DIR, DATA_DIR, SCRIPTS_DIR #NOTEBOOKS_DIR, RESOURCES_DINFO_DIR, PREPROCESSING_NOTES_DIR, \n",
    "print('++ INFO: Project Dir:                  %s' % PRJ_DIR) \n",
    "#print('++ INFO: Notebooks Dir:                %s' % NOTEBOOKS_DIR) \n",
    "print('++ INFO: Bash Scripts Dir:             %s' % SCRIPTS_DIR)\n",
    "#print('++ INFO: Resources (Dataset Info) Dir: %s' % RESOURCES_DINFO_DIR)\n",
    "#print('++ INFO: Pre-processing Notes Dir:     %s' % PREPROCESSING_NOTES_DIR)\n",
    "print('++ INFO: Data Dir:                     %s' % DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5413dce3-0112-4d37-8fc1-31319a158ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: user working now --> javiergc\n"
     ]
    }
   ],
   "source": [
    "username = getpass.getuser()\n",
    "print('++ INFO: user working now --> %s' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c30d5a0-3119-4fba-9cb7-09f6515156df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user specific folders\n",
    "#=====================\n",
    "swarm_folder   = osp.join(PRJ_DIR,'SwarmFiles.{username}'.format(username=username))\n",
    "logs_folder    = osp.join(PRJ_DIR,'Logs.{username}'.format(username=username))\n",
    "\n",
    "swarm_path     = osp.join(swarm_folder,'S19_NiMareTopics.SWARM.sh')\n",
    "logdir_path    = osp.join(logs_folder, 'S19_NiMareTopics.logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae2109f-2e27-42f0-9a69-83ef641607e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: New folder for log files created [/data/SFIMJGC_Introspec/2023_fc_introspection/Logs.javiergc/S19_NiMareTopics.logs]\n"
     ]
    }
   ],
   "source": [
    "# create user specific folders if needed\n",
    "# ======================================\n",
    "if not osp.exists(swarm_folder):\n",
    "    os.makedirs(swarm_folder)\n",
    "    print('++ INFO: New folder for swarm files created [%s]' % swarm_folder)\n",
    "if not osp.exists(logdir_path):\n",
    "    os.makedirs(logdir_path)\n",
    "    print('++ INFO: New folder for log files created [%s]' % logdir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28eafb4b-cd2c-41a1-b3f7-1aed749eb828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "swarm_file = open(swarm_path, \"w\")\n",
    "# Log the date and time when the SWARM file is created\n",
    "swarm_file.write('#Create Time: %s' % datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "swarm_file.write('\\n')\n",
    "# Insert comment line with SWARM command\n",
    "swarm_file.write('#swarm -f {swarm_path} -g 32 -t 8 --partition quick,norm -b 5 --time 00:48:00 --logdir {logdir_path}'.format(swarm_path=swarm_path,logdir_path=logdir_path))\n",
    "swarm_file.write('\\n')\n",
    "\n",
    "# Insert one line per subject\n",
    "for topic in topics:\n",
    "    swarm_file.write(f\"export VOCAB={vocab} TOPIC={topic}; sh {SCRIPTS_DIR}/S19_NiMare_Create_TopicMaps.sh\")\n",
    "    swarm_file.write('\\n')\n",
    "swarm_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337c2ca5-cc6a-482b-9e43-abdf8372864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/SFIMJGC_Introspec/2023_fc_introspection/SwarmFiles.javiergc/S19_NiMareTopics.SWARM.sh\n"
     ]
    }
   ],
   "source": [
    "print(swarm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45f2db-25a3-4b3a-ad62-b255222cfc56",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c439254f-a924-4e3c-b116-bb9d862045b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimare import annotate\n",
    "from nimare import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1f0f7be-3aad-4f26-aa63-892c26d52287",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_file    = os.path.join(RESOURCE_DIR, \"neurosynth_dataset.pkl.gz\")\n",
    "dset_ma_file = os.path.join(RESOURCE_DIR, \"neurosynth_dataset_with_ma.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "946018ad-0409-4a7e-a785-03a04a8ca5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_ma = dataset.Dataset.load(dset_ma_file)\n",
    "dset = dataset.Dataset.load(dset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54abccc3-48b7-47b5-9696-8432d8b216e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = annotate.lda.LDAModel(n_topics=50,max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22fbbbbe-38fa-4b05-b832-254755ece034",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35453/1466653714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Dataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "dset['vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b17d89-a60c-4fd8-b55d-e2e74eb3447c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column 'abstract' not found in DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nimare/annotate/lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dset)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    110\u001b[0m         \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/SFIMJGC_HCP7T/Apps/envs/fc_introspection/lib/python3.7/site-packages/nimare/annotate/text.py\u001b[0m in \u001b[0;36mgenerate_counts\u001b[0;34m(text_df, text_column, tfidf, min_df, max_df)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext_column\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column '{text_column}' not found in DataFrame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Remove rows with empty text cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Column 'abstract' not found in DataFrame"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_dset = model.fit(dset_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b26d01-44a3-4240-9fb6-979f27ea6e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mask': None, 'source': None, 'target': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurosynth_dset.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366129f8-098d-47b2-b13a-043887be40f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7071718-d755-478f-a38b-e7a668d22bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b850d7a-84a6-4ae0-bf65-6c054d58a7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba0244-dbcf-49e6-ad82-f97f91005198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455e081-f387-4519-a10a-6cf0a308c9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8d89f-355f-4136-bb9e-9ef0df22279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Get Dictionary topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topics:   0%|▋                                                                                                                                                                                                                                                                    | 1/400 [02:03<13:37:59, 123.01s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get features\n",
    "print (\"++ INFO: Get Dictionary topics\")\n",
    "topics = neurosynth_dset.get_labels()\n",
    "useful_topics = []\n",
    "for topic in tqdm(topics, desc='Topics'):\n",
    "    # Use a threshold of 0.05 for topics, even though the default threshold is 0.001.\n",
    "    topic_positive_ids = neurosynth_dset.get_studies_by_label(topic, 0.05)\n",
    "    topic_negative_ids = list(set(neurosynth_dset.ids) - set(topic_positive_ids))\n",
    "\n",
    "    # Require some minimum number of studies in each sample\n",
    "    if (len(topic_positive_ids) == 0) or (len(topic_negative_ids) == 0):\n",
    "        print(\"++ [WARNING] Skipping {topic}\".format(topic=topic))\n",
    "        continue\n",
    "    useful_topics.append(topic)\n",
    "\n",
    "    topic_positive_dset = neurosynth_dset.slice(topic_positive_ids)\n",
    "    topic_negative_dset = neurosynth_dset.slice(topic_negative_ids)\n",
    "    \n",
    "    # MEMORY HEAVE LINE\n",
    "    meta_result = meta_estimator.fit(topic_positive_dset, topic_negative_dset)\n",
    "    # Save the maps to an output directory\n",
    "    #print(\" +         Saving maps for topic [%s]\" % str(topic))\n",
    "    meta_result.save_maps(output_dir=METAMAPS_DIR, prefix=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8232e886-3ac8-4fd9-b950-51d5d67b88dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nimare.results.MetaResult at 0x2aaab1ef2c50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc3b9b46-0a9e-48bd-9be6-4c7f0534e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_result.save_maps(output_dir=METAMAPS_DIR, prefix=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a2d65f1-aa42-4f8f-9502-71ab0d4fb0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LDA400_abstract_weight__0_exercise_igd_internet',\n",
       " 'LDA400_abstract_weight__1_context_contextual_contexts',\n",
       " 'LDA400_abstract_weight__2_sulcus_intraparietal_ips',\n",
       " 'LDA400_abstract_weight__3_individuals_resonance_magnetic',\n",
       " 'LDA400_abstract_weight__4_mood_affective_induction',\n",
       " 'LDA400_abstract_weight__5_judgments_judgment_judged',\n",
       " 'LDA400_abstract_weight__6_long_term_short',\n",
       " 'LDA400_abstract_weight__7_passive_viewing_listening',\n",
       " 'LDA400_abstract_weight__8_arousal_subjective_ratings',\n",
       " 'LDA400_abstract_weight__9_amplitude_spontaneous_frequency',\n",
       " 'LDA400_abstract_weight__10_timing_rhythm_beat',\n",
       " 'LDA400_abstract_weight__11_children_adults_age',\n",
       " 'LDA400_abstract_weight__12_structures_pattern_contrast',\n",
       " 'LDA400_abstract_weight__13_motion_mt_visual',\n",
       " 'LDA400_abstract_weight__14_central_capacity_nervous',\n",
       " 'LDA400_abstract_weight__15_pattern_patterns_multivariate',\n",
       " 'LDA400_abstract_weight__16_executive_control_cognitive',\n",
       " 'LDA400_abstract_weight__17_pain_painful_chronic',\n",
       " 'LDA400_abstract_weight__18_amygdala_reactivity_affective',\n",
       " 'LDA400_abstract_weight__19_mechanisms_underlying_unknown',\n",
       " 'LDA400_abstract_weight__20_emotional_emotion_amygdala',\n",
       " 'LDA400_abstract_weight__21_strategies_strategy_strategic',\n",
       " 'LDA400_abstract_weight__22_thalamus_insula_putamen',\n",
       " 'LDA400_abstract_weight__23_reading_letter_chinese',\n",
       " 'LDA400_abstract_weight__24_identification_view_views',\n",
       " 'LDA400_abstract_weight__25_approach_method_techniques',\n",
       " 'LDA400_abstract_weight__26_ifg_gyrus_frontal',\n",
       " 'LDA400_abstract_weight__27_gaba_ratio_levels',\n",
       " 'LDA400_abstract_weight__28_sma_pre_motor',\n",
       " 'LDA400_abstract_weight__29_response_hemodynamic_time',\n",
       " 'LDA400_abstract_weight__30_cbf_asl_epi',\n",
       " 'LDA400_abstract_weight__31_stimulus_response_type',\n",
       " 'LDA400_abstract_weight__32_fear_conditioning_extinction',\n",
       " 'LDA400_abstract_weight__33_network_identified_core',\n",
       " 'LDA400_abstract_weight__34_monitoring_source_reality',\n",
       " 'LDA400_abstract_weight__35_test_tests_cognitive',\n",
       " 'LDA400_abstract_weight__36_frequency_hz_slow',\n",
       " 'LDA400_abstract_weight__37_anticipation_anticipatory_aversive',\n",
       " 'LDA400_abstract_weight__38_post_pre_ht',\n",
       " 'LDA400_abstract_weight__39_dmpfc_balance_dorsomedial',\n",
       " 'LDA400_abstract_weight__40_focus_multi_bci',\n",
       " 'LDA400_abstract_weight__41_frontoparietal_subregions_network',\n",
       " 'LDA400_abstract_weight__42_ibs_visceral_rectal',\n",
       " 'LDA400_abstract_weight__43_task_recruitment_recruited',\n",
       " 'LDA400_abstract_weight__44_parietal_temporal_frontal',\n",
       " 'LDA400_abstract_weight__45_training_trained_transfer',\n",
       " 'LDA400_abstract_weight__46_neuronal_single_relevance',\n",
       " 'LDA400_abstract_weight__47_fc_resting_state',\n",
       " 'LDA400_abstract_weight__48_faces_amygdala_emotional',\n",
       " 'LDA400_abstract_weight__49_pmc_cord_sci',\n",
       " 'LDA400_abstract_weight__50_color_shape_shapes',\n",
       " 'LDA400_abstract_weight__51_route_recognition_forms',\n",
       " 'LDA400_abstract_weight__52_pd_disease_parkinson',\n",
       " 'LDA400_abstract_weight__53_ec_sp_eo',\n",
       " 'LDA400_abstract_weight__54_awareness_conscious_consciousness',\n",
       " 'LDA400_abstract_weight__55_perceptual_discrimination_perception',\n",
       " 'LDA400_abstract_weight__56_cortex_prefrontal_orbitofrontal',\n",
       " 'LDA400_abstract_weight__57_presented_visual_presentation',\n",
       " 'LDA400_abstract_weight__58_autonomic_skin_arousal',\n",
       " 'LDA400_abstract_weight__59_writing_drawing_figure',\n",
       " 'LDA400_abstract_weight__60_rate_heart_cardiovascular',\n",
       " 'LDA400_abstract_weight__61_recall_digit_span',\n",
       " 'LDA400_abstract_weight__62_familiar_unfamiliar_familiarity',\n",
       " 'LDA400_abstract_weight__63_negative_positive_valence',\n",
       " 'LDA400_abstract_weight__64_selection_pmd_rostral',\n",
       " 'LDA400_abstract_weight__65_pfc_prefrontal_cortex',\n",
       " 'LDA400_abstract_weight__66_fine_fcd_grained',\n",
       " 'LDA400_abstract_weight__67_cortex_lateral_prefrontal',\n",
       " 'LDA400_abstract_weight__68_scenes_scene_ppa',\n",
       " 'LDA400_abstract_weight__69_cortex_primary_somatosensory',\n",
       " 'LDA400_abstract_weight__70_mpfc_medial_prefrontal',\n",
       " 'LDA400_abstract_weight__71_dimensions_horizontal_ap',\n",
       " 'LDA400_abstract_weight__72_cognitive_control_cognition',\n",
       " 'LDA400_abstract_weight__73_function_structure_magnetic',\n",
       " 'LDA400_abstract_weight__74_fg_values_dbs',\n",
       " 'LDA400_abstract_weight__75_motor_cortex_supplementary',\n",
       " 'LDA400_abstract_weight__76_als_scores_students',\n",
       " 'LDA400_abstract_weight__77_adhd_disorder_attention',\n",
       " 'LDA400_abstract_weight__78_nc_abeta_amyloid',\n",
       " 'LDA400_abstract_weight__79_users_cocaine_drug',\n",
       " 'LDA400_abstract_weight__80_adolescents_adolescent_youth',\n",
       " 'LDA400_abstract_weight__81_vmpfc_ventromedial_prefrontal',\n",
       " 'LDA400_abstract_weight__82_comprehension_sentences_language',\n",
       " 'LDA400_abstract_weight__83_mapping_scanner_mr',\n",
       " 'LDA400_abstract_weight__84_real_world_nf',\n",
       " 'LDA400_abstract_weight__85_images_image_noise',\n",
       " 'LDA400_abstract_weight__86_number_numerical_numbers',\n",
       " 'LDA400_abstract_weight__87_psychosis_risk_schizophrenia',\n",
       " 'LDA400_abstract_weight__88_response_stimulus_contrast',\n",
       " 'LDA400_abstract_weight__89_range_wide_characteristic',\n",
       " 'LDA400_abstract_weight__90_perception_perceived_subliminal',\n",
       " 'LDA400_abstract_weight__91_age_years_group',\n",
       " 'LDA400_abstract_weight__92_syndrome_ts_japanese',\n",
       " 'LDA400_abstract_weight__93_speed_separation_mph',\n",
       " 'LDA400_abstract_weight__94_events_future_past',\n",
       " 'LDA400_abstract_weight__95_eye_saccade_saccades',\n",
       " 'LDA400_abstract_weight__96_alexithymia_tas_ks',\n",
       " 'LDA400_abstract_weight__97_evaluation_evaluations_esteem',\n",
       " 'LDA400_abstract_weight__98_stroke_recovery_acute',\n",
       " 'LDA400_abstract_weight__99_delay_delayed_discounting',\n",
       " 'LDA400_abstract_weight__100_process_tracking_run',\n",
       " 'LDA400_abstract_weight__101_feedback_negative_performance',\n",
       " 'LDA400_abstract_weight__102_hippocampal_hippocampus_memory',\n",
       " 'LDA400_abstract_weight__103_types_type_similarity',\n",
       " 'LDA400_abstract_weight__104_trials_trial_response',\n",
       " 'LDA400_abstract_weight__105_motivation_avoidance_approach',\n",
       " 'LDA400_abstract_weight__106_subgroups_severe_mild',\n",
       " 'LDA400_abstract_weight__107_imagery_mental_imagined',\n",
       " 'LDA400_abstract_weight__108_tom_mind_theory',\n",
       " 'LDA400_abstract_weight__109_tle_surgical_sm',\n",
       " 'LDA400_abstract_weight__110_adaptation_repeated_code',\n",
       " 'LDA400_abstract_weight__111_pictures_neutral_picture',\n",
       " 'LDA400_abstract_weight__112_error_errors_monitoring',\n",
       " 'LDA400_abstract_weight__113_spatial_method_subject',\n",
       " 'LDA400_abstract_weight__114_measures_measure_total',\n",
       " 'LDA400_abstract_weight__115_wm_memory_working',\n",
       " 'LDA400_abstract_weight__116_motor_cortex_sensorimotor',\n",
       " 'LDA400_abstract_weight__117_food_eating_weight',\n",
       " 'LDA400_abstract_weight__118_striatum_striatal_ventral',\n",
       " 'LDA400_abstract_weight__119_temporal_lobe_anterior',\n",
       " 'LDA400_abstract_weight__120_verbal_fluency_task',\n",
       " 'LDA400_abstract_weight__121_abnormal_disorder_abnormalities',\n",
       " 'LDA400_abstract_weight__122_modulation_modulated_modulate',\n",
       " 'LDA400_abstract_weight__123_words_word_lexical',\n",
       " 'LDA400_abstract_weight__124_cerebellar_cerebellum_ii',\n",
       " 'LDA400_abstract_weight__125_mhe_cirrhotic_lt',\n",
       " 'LDA400_abstract_weight__126_stimulation_somatosensory_representation',\n",
       " 'LDA400_abstract_weight__127_patterns_properties_strongly',\n",
       " 'LDA400_abstract_weight__128_problem_problems_arithmetic',\n",
       " 'LDA400_abstract_weight__129_gm_volume_matter',\n",
       " 'LDA400_abstract_weight__130_frontal_inferior_gyrus',\n",
       " 'LDA400_abstract_weight__131_inhibition_response_stop',\n",
       " 'LDA400_abstract_weight__132_cues_cue_cued',\n",
       " 'LDA400_abstract_weight__133_video_clips_viewing',\n",
       " 'LDA400_abstract_weight__134_parietal_ppc_posterior',\n",
       " 'LDA400_abstract_weight__135_internal_external_intention',\n",
       " 'LDA400_abstract_weight__136_insight_hum_mapp',\n",
       " 'LDA400_abstract_weight__137_blind_sighted_global',\n",
       " 'LDA400_abstract_weight__138_ketamine_intervals_interval',\n",
       " 'LDA400_abstract_weight__139_white_matter_tensor',\n",
       " 'LDA400_abstract_weight__140_finger_tapping_index',\n",
       " 'LDA400_abstract_weight__141_task_performing_cognitive',\n",
       " 'LDA400_abstract_weight__142_lesions_lesion_patient',\n",
       " 'LDA400_abstract_weight__143_taste_swallowing_gustatory',\n",
       " 'LDA400_abstract_weight__144_decision_making_choice',\n",
       " 'LDA400_abstract_weight__145_gyrus_temporal_frontal',\n",
       " 'LDA400_abstract_weight__146_creative_creativity_thinking',\n",
       " 'LDA400_abstract_weight__147_sequence_sequences_order',\n",
       " 'LDA400_abstract_weight__148_beauty_aesthetic_art',\n",
       " 'LDA400_abstract_weight__149_intervention_therapy_cbt',\n",
       " 'LDA400_abstract_weight__150_interaction_main_session',\n",
       " 'LDA400_abstract_weight__151_loss_gain_losses',\n",
       " 'LDA400_abstract_weight__152_reference_frame_relations',\n",
       " 'LDA400_abstract_weight__153_women_men_sex',\n",
       " 'LDA400_abstract_weight__154_parietal_network_fronto',\n",
       " 'LDA400_abstract_weight__155_perspective_person_egocentric',\n",
       " 'LDA400_abstract_weight__156_practice_generation_retention',\n",
       " 'LDA400_abstract_weight__157_repetition_priming_repeated',\n",
       " 'LDA400_abstract_weight__158_significance_principal_methodology',\n",
       " 'LDA400_abstract_weight__159_hemisphere_hemispheric_lateralization',\n",
       " 'LDA400_abstract_weight__160_sustained_transient_onset',\n",
       " 'LDA400_abstract_weight__161_smokers_smoking_nicotine',\n",
       " 'LDA400_abstract_weight__162_race_cultural_chinese',\n",
       " 'LDA400_abstract_weight__163_face_faces_fusiform',\n",
       " 'LDA400_abstract_weight__164_incompatible_compatibility_tmt',\n",
       " 'LDA400_abstract_weight__165_instruction_instructed_instructions',\n",
       " 'LDA400_abstract_weight__166_size_small_large',\n",
       " 'LDA400_abstract_weight__167_state_resting_seed',\n",
       " 'LDA400_abstract_weight__168_tool_object_hand',\n",
       " 'LDA400_abstract_weight__169_gyrus_fusiform_parahippocampal',\n",
       " 'LDA400_abstract_weight__170_body_bodies_eba',\n",
       " 'LDA400_abstract_weight__171_task_switching_set',\n",
       " 'LDA400_abstract_weight__172_anterior_insula_cortex',\n",
       " 'LDA400_abstract_weight__173_network_default_dmn',\n",
       " 'LDA400_abstract_weight__174_music_musical_musicians',\n",
       " 'LDA400_abstract_weight__175_acupuncture_stimulation_sa',\n",
       " 'LDA400_abstract_weight__176_movements_movement_motor',\n",
       " 'LDA400_abstract_weight__177_ms_source_sources',\n",
       " 'LDA400_abstract_weight__178_anxiety_trait_anxious',\n",
       " 'LDA400_abstract_weight__179_resonance_magnetic_response',\n",
       " 'LDA400_abstract_weight__180_speech_sounds_auditory',\n",
       " 'LDA400_abstract_weight__181_females_males_sex',\n",
       " 'LDA400_abstract_weight__182_early_stage_stages',\n",
       " 'LDA400_abstract_weight__183_moral_psychopathy_harm',\n",
       " 'LDA400_abstract_weight__184_association_relationship_relationships',\n",
       " 'LDA400_abstract_weight__185_maps_space_template',\n",
       " 'LDA400_abstract_weight__186_cortex_visual_temporal',\n",
       " 'LDA400_abstract_weight__187_parietal_inferior_lobule',\n",
       " 'LDA400_abstract_weight__188_prior_initial_time',\n",
       " 'LDA400_abstract_weight__189_risk_risky_taking',\n",
       " 'LDA400_abstract_weight__190_outcome_born_outcomes',\n",
       " 'LDA400_abstract_weight__191_stimulation_rtms_tdcs',\n",
       " 'LDA400_abstract_weight__192_pet_cerebral_rcbf',\n",
       " 'LDA400_abstract_weight__193_language_hemisphere_linguistic',\n",
       " 'LDA400_abstract_weight__194_gyrus_ag_angular',\n",
       " 'LDA400_abstract_weight__195_alcohol_substance_impulsivity',\n",
       " 'LDA400_abstract_weight__196_uncertainty_ambiguous_ambiguity',\n",
       " 'LDA400_abstract_weight__197_stress_cortisol_response',\n",
       " 'LDA400_abstract_weight__198_ad_mci_disease',\n",
       " 'LDA400_abstract_weight__199_versus_implicated_correlates',\n",
       " 'LDA400_abstract_weight__200_load_task_high',\n",
       " 'LDA400_abstract_weight__201_speech_auditory_prosody',\n",
       " 'LDA400_abstract_weight__202_classification_accuracy_machine',\n",
       " 'LDA400_abstract_weight__203_resonance_magnetic_control',\n",
       " 'LDA400_abstract_weight__204_faces_face_facial',\n",
       " 'LDA400_abstract_weight__205_task_performance_rest',\n",
       " 'LDA400_abstract_weight__206_apoe_epsilon_risk',\n",
       " 'LDA400_abstract_weight__207_target_targets_distractor',\n",
       " 'LDA400_abstract_weight__208_reaction_time_times',\n",
       " 'LDA400_abstract_weight__209_exclusion_ds_rejection',\n",
       " 'LDA400_abstract_weight__210_control_inhibitory_attentional',\n",
       " 'LDA400_abstract_weight__211_model_models_theory',\n",
       " 'LDA400_abstract_weight__212_bpd_disorder_personality',\n",
       " 'LDA400_abstract_weight__213_sexual_love_romantic',\n",
       " 'LDA400_abstract_weight__214_performance_task_improved',\n",
       " 'LDA400_abstract_weight__215_dopamine_da_receptor',\n",
       " 'LDA400_abstract_weight__216_frontal_inferior_gyrus',\n",
       " 'LDA400_abstract_weight__217_tms_stimulation_magnetic',\n",
       " 'LDA400_abstract_weight__218_sz_vta_substantia',\n",
       " 'LDA400_abstract_weight__219_sd_pupil_exception',\n",
       " 'LDA400_abstract_weight__220_question_unique_questions',\n",
       " 'LDA400_abstract_weight__221_systems_correlates_linked',\n",
       " 'LDA400_abstract_weight__222_interference_stroop_control',\n",
       " 'LDA400_abstract_weight__223_humor_lc_na',\n",
       " 'LDA400_abstract_weight__224_limbic_amygdala_paralimbic',\n",
       " 'LDA400_abstract_weight__225_olfactory_odor_odors',\n",
       " 'LDA400_abstract_weight__226_physical_itch_sc',\n",
       " 'LDA400_abstract_weight__227_cortex_anterior_cingulate',\n",
       " 'LDA400_abstract_weight__228_independent_model_approach',\n",
       " 'LDA400_abstract_weight__229_mtl_recollection_memory',\n",
       " 'LDA400_abstract_weight__230_ventral_dorsal_visual',\n",
       " 'LDA400_abstract_weight__231_age_young_adults',\n",
       " 'LDA400_abstract_weight__232_hand_hands_foot',\n",
       " 'LDA400_abstract_weight__233_category_categories_categorization',\n",
       " 'LDA400_abstract_weight__234_learning_learned_associations',\n",
       " 'LDA400_abstract_weight__235_pairs_associative_associations',\n",
       " 'LDA400_abstract_weight__236_social_cognition_interactions',\n",
       " 'LDA400_abstract_weight__237_medial_prefrontal_cortex',\n",
       " 'LDA400_abstract_weight__238_adults_older_age',\n",
       " 'LDA400_abstract_weight__239_ptsd_trauma_stress',\n",
       " 'LDA400_abstract_weight__240_performance_task_cognitive',\n",
       " 'LDA400_abstract_weight__241_expertise_experts_novices',\n",
       " 'LDA400_abstract_weight__242_set_identified_sets',\n",
       " 'LDA400_abstract_weight__243_prefrontal_cortex_dorsolateral',\n",
       " 'LDA400_abstract_weight__244_game_social_trust',\n",
       " 'LDA400_abstract_weight__245_gaze_eye_eyes',\n",
       " 'LDA400_abstract_weight__246_interaction_ppi_psychophysiological',\n",
       " 'LDA400_abstract_weight__247_treatment_baseline_follow',\n",
       " 'LDA400_abstract_weight__248_detection_novelty_oddball',\n",
       " 'LDA400_abstract_weight__249_phase_phases_pws',\n",
       " 'LDA400_abstract_weight__250_gestures_gesture_communicative',\n",
       " 'LDA400_abstract_weight__251_likelihood_consistency_estimation',\n",
       " 'LDA400_abstract_weight__252_relational_strength_strong',\n",
       " 'LDA400_abstract_weight__253_age_development_adolescence',\n",
       " 'LDA400_abstract_weight__254_empathy_social_empathic',\n",
       " 'LDA400_abstract_weight__255_action_actions_observation',\n",
       " 'LDA400_abstract_weight__256_encoding_memory_recognition',\n",
       " 'LDA400_abstract_weight__257_preference_preferences_nirs',\n",
       " 'LDA400_abstract_weight__258_human_provide_input',\n",
       " 'LDA400_abstract_weight__259_sentences_sentence_syntactic',\n",
       " 'LDA400_abstract_weight__260_social_partner_interaction',\n",
       " 'LDA400_abstract_weight__261_risk_genetic_relatives',\n",
       " 'LDA400_abstract_weight__262_allele_genotype_met',\n",
       " 'LDA400_abstract_weight__263_selective_texture_selectivity',\n",
       " 'LDA400_abstract_weight__264_attention_attentional_orienting',\n",
       " 'LDA400_abstract_weight__265_magnetic_resonance_level',\n",
       " 'LDA400_abstract_weight__266_ofc_orbitofrontal_cortex',\n",
       " 'LDA400_abstract_weight__267_injury_tbi_traumatic',\n",
       " 'LDA400_abstract_weight__268_stimulation_somatosensory_contralateral',\n",
       " 'LDA400_abstract_weight__269_mi_withdrawal_sfc',\n",
       " 'LDA400_abstract_weight__270_force_motor_grip',\n",
       " 'LDA400_abstract_weight__271_expectancy_sensory_expectation',\n",
       " 'LDA400_abstract_weight__272_retrieval_memory_episodic',\n",
       " 'LDA400_abstract_weight__273_threat_fear_anxiety',\n",
       " 'LDA400_abstract_weight__274_naming_production_overt',\n",
       " 'LDA400_abstract_weight__275_power_gamma_hz',\n",
       " 'LDA400_abstract_weight__276_tactile_synchrony_synchronous',\n",
       " 'LDA400_abstract_weight__277_mirror_imitation_observation',\n",
       " 'LDA400_abstract_weight__278_prediction_predictive_predictions',\n",
       " 'LDA400_abstract_weight__279_bimanual_sem_unimanual',\n",
       " 'LDA400_abstract_weight__280_pcc_cingulate_precuneus',\n",
       " 'LDA400_abstract_weight__281_gmv_dystonia_pls',\n",
       " 'LDA400_abstract_weight__282_illusion_illusory_contour',\n",
       " 'LDA400_abstract_weight__283_correct_successful_incorrect',\n",
       " 'LDA400_abstract_weight__284_language_english_native',\n",
       " 'LDA400_abstract_weight__285_junction_tpj_temporoparietal',\n",
       " 'LDA400_abstract_weight__286_competition_free_selection',\n",
       " 'LDA400_abstract_weight__287_navigation_virtual_gait',\n",
       " 'LDA400_abstract_weight__288_mental_rotation_visuospatial',\n",
       " 'LDA400_abstract_weight__289_model_vestibular_dcm',\n",
       " 'LDA400_abstract_weight__290_tinnitus_mindfulness_meditation',\n",
       " 'LDA400_abstract_weight__291_infant_attachment_child',\n",
       " 'LDA400_abstract_weight__292_task_relevant_irrelevant',\n",
       " 'LDA400_abstract_weight__293_temporal_sts_superior',\n",
       " 'LDA400_abstract_weight__294_surface_colour_thinning',\n",
       " 'LDA400_abstract_weight__295_previous_research_reported',\n",
       " 'LDA400_abstract_weight__296_effort_fnc_reporting',\n",
       " 'LDA400_abstract_weight__297_attention_attentional_attended',\n",
       " 'LDA400_abstract_weight__298_asd_autism_spectrum',\n",
       " 'LDA400_abstract_weight__299_memory_working_verbal',\n",
       " 'LDA400_abstract_weight__300_reliability_test_retest',\n",
       " 'LDA400_abstract_weight__301_voice_vocal_voices',\n",
       " 'LDA400_abstract_weight__302_touch_tactile_somatosensory',\n",
       " 'LDA400_abstract_weight__303_rs_pa_hyperalgesia',\n",
       " 'LDA400_abstract_weight__304_epilepsy_lobe_temporal',\n",
       " 'LDA400_abstract_weight__305_updating_cold_endogenous',\n",
       " 'LDA400_abstract_weight__306_understood_poorly_mechanisms',\n",
       " 'LDA400_abstract_weight__307_experience_subjective_experiences',\n",
       " 'LDA400_abstract_weight__308_sleep_consolidation_deprivation',\n",
       " 'LDA400_abstract_weight__309_ocd_disorder_compulsive',\n",
       " 'LDA400_abstract_weight__310_common_suggest_overlap',\n",
       " 'LDA400_abstract_weight__311_knowledge_attributes_semantic',\n",
       " 'LDA400_abstract_weight__312_placebo_mg_blind',\n",
       " 'LDA400_abstract_weight__313_matter_gray_volume',\n",
       " 'LDA400_abstract_weight__314_people_person_situation',\n",
       " 'LDA400_abstract_weight__315_status_engaged_thought',\n",
       " 'LDA400_abstract_weight__316_sensitive_sensitivity_suggest',\n",
       " 'LDA400_abstract_weight__317_insula_disgust_insular',\n",
       " 'LDA400_abstract_weight__318_female_male_gender',\n",
       " 'LDA400_abstract_weight__319_individual_variability_inter',\n",
       " 'LDA400_abstract_weight__320_organization_hierarchical_organized',\n",
       " 'LDA400_abstract_weight__321_important_play_suggest',\n",
       " 'LDA400_abstract_weight__322_task_difficulty_demands',\n",
       " 'LDA400_abstract_weight__323_cerebral_ataxia_sca',\n",
       " 'LDA400_abstract_weight__324_bias_cr_biases',\n",
       " 'LDA400_abstract_weight__325_intelligence_md_fluid',\n",
       " 'LDA400_abstract_weight__326_predicted_individual_predict',\n",
       " 'LDA400_abstract_weight__327_basal_ganglia_thalamus',\n",
       " 'LDA400_abstract_weight__328_weight_lh_responsivity',\n",
       " 'LDA400_abstract_weight__329_items_recognition_item',\n",
       " 'LDA400_abstract_weight__330_salience_network_sn',\n",
       " 'LDA400_abstract_weight__331_reasoning_hearing_deaf',\n",
       " 'LDA400_abstract_weight__332_ms_sclerosis_multiple',\n",
       " 'LDA400_abstract_weight__333_incongruent_congruent_congruency',\n",
       " 'LDA400_abstract_weight__334_spatial_location_locations',\n",
       " 'LDA400_abstract_weight__335_regulation_emotion_reappraisal',\n",
       " 'LDA400_abstract_weight__336_personality_traits_trait',\n",
       " 'LDA400_abstract_weight__337_suppression_voluntary_suppressed',\n",
       " 'LDA400_abstract_weight__338_reward_striatum_monetary',\n",
       " 'LDA400_abstract_weight__339_rule_rules_abstraction',\n",
       " 'LDA400_abstract_weight__340_lsf_bladder_hsf',\n",
       " 'LDA400_abstract_weight__341_matching_match_task',\n",
       " 'LDA400_abstract_weight__342_caudate_nucleus_accumbens',\n",
       " 'LDA400_abstract_weight__343_light_shed_impact',\n",
       " 'LDA400_abstract_weight__344_disease_atrophy_dementia',\n",
       " 'LDA400_abstract_weight__345_roi_voxel_rois',\n",
       " 'LDA400_abstract_weight__346_matter_grey_volume',\n",
       " 'LDA400_abstract_weight__347_high_level_resolution',\n",
       " 'LDA400_abstract_weight__348_facial_emotion_expressions',\n",
       " 'LDA400_abstract_weight__349_visual_occipital_early',\n",
       " 'LDA400_abstract_weight__350_brainstem_pag_ac',\n",
       " 'LDA400_abstract_weight__351_symptoms_severity_scores',\n",
       " 'LDA400_abstract_weight__352_linear_relationship_model',\n",
       " 'LDA400_abstract_weight__353_deception_truth_lying',\n",
       " 'LDA400_abstract_weight__354_depression_mdd_depressive',\n",
       " 'LDA400_abstract_weight__355_hallucinations_auditory_avh',\n",
       " 'LDA400_abstract_weight__356_semantic_word_temporal',\n",
       " 'LDA400_abstract_weight__357_verbs_verb_nouns',\n",
       " 'LDA400_abstract_weight__358_autobiographical_memories_memory',\n",
       " 'LDA400_abstract_weight__359_goal_goals_ma',\n",
       " 'LDA400_abstract_weight__360_pars_opercularis_inferior',\n",
       " 'LDA400_abstract_weight__361_mechanisms_automatic_underlying',\n",
       " 'LDA400_abstract_weight__362_schizophrenia_abnormalities_symptoms',\n",
       " 'LDA400_abstract_weight__363_dacc_dorsal_cingulate',\n",
       " 'LDA400_abstract_weight__364_network_graph_local',\n",
       " 'LDA400_abstract_weight__365_time_rt_reaction',\n",
       " 'LDA400_abstract_weight__366_mental_mentalizing_states',\n",
       " 'LDA400_abstract_weight__367_human_agents_agent',\n",
       " 'LDA400_abstract_weight__368_pathway_pathways_indirect',\n",
       " 'LDA400_abstract_weight__369_fitness_heuristic_pbd',\n",
       " 'LDA400_abstract_weight__370_acc_cingulate_anterior',\n",
       " 'LDA400_abstract_weight__371_search_lifg_frontal',\n",
       " 'LDA400_abstract_weight__372_group_control_individuals',\n",
       " 'LDA400_abstract_weight__373_manipulation_magnitude_simple',\n",
       " 'LDA400_abstract_weight__374_magnetic_resonance_suggest',\n",
       " 'LDA400_abstract_weight__375_integration_audiovisual_visual',\n",
       " 'LDA400_abstract_weight__376_space_distance_line',\n",
       " 'LDA400_abstract_weight__377_rsfc_state_resting',\n",
       " 'LDA400_abstract_weight__378_visual_auditory_modality',\n",
       " 'LDA400_abstract_weight__379_auditory_sound_cortex',\n",
       " 'LDA400_abstract_weight__380_reho_regional_homogeneity',\n",
       " 'LDA400_abstract_weight__381_implicit_explicit_task',\n",
       " 'LDA400_abstract_weight__382_influence_cortex_influences',\n",
       " 'LDA400_abstract_weight__383_abstract_concrete_concepts',\n",
       " 'LDA400_abstract_weight__384_object_objects_recognition',\n",
       " 'LDA400_abstract_weight__385_provide_patterns_profile',\n",
       " 'LDA400_abstract_weight__386_thalamic_thalamus_pulvinar',\n",
       " 'LDA400_abstract_weight__387_trial_single_task',\n",
       " 'LDA400_abstract_weight__388_conflict_response_monitoring',\n",
       " 'LDA400_abstract_weight__389_intrinsic_extrinsic_hp',\n",
       " 'LDA400_abstract_weight__390_representations_representation_cortex',\n",
       " 'LDA400_abstract_weight__391_higher_level_lower',\n",
       " 'LDA400_abstract_weight__392_reading_phonological_dyslexia',\n",
       " 'LDA400_abstract_weight__393_increasing_level_time',\n",
       " 'LDA400_abstract_weight__394_identified_independent_multiple',\n",
       " 'LDA400_abstract_weight__395_hr_physiological_hypothalamus',\n",
       " 'LDA400_abstract_weight__396_human_humans_animal',\n",
       " 'LDA400_abstract_weight__397_performance_impaired_poor',\n",
       " 'LDA400_abstract_weight__398_dlpfc_prefrontal_cortex',\n",
       " 'LDA400_abstract_weight__399_bipolar_bd_disorder']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c8fb0-db3a-46b9-9759-5b9f349f384d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FC Introspection (Jan 2023)",
   "language": "python",
   "name": "fc_introspection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
