{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117754e3-6f61-42aa-8d2c-c637c34dd1bb",
   "metadata": {},
   "source": [
    "# Description - Prepare Data for NBS Analysis on Matlab\n",
    "\n",
    "This notebook prepares connectivity matrices for doing the NBS analyses on Matlab. At this point we have selected scans corresponding to the two populations that we want to compare and all that's left is to generate the design matrices and create a copy of the FC matrices in a format that NBS can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0e4807-60d4-48ed-aa5a-f1e7eca31d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from sfim_lib.io.afni import load_netcc\n",
    "from shutil import rmtree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b35d4ec-c935-4158-97e8-12434d037a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.basics import RESOURCES_NBS_DIR, ATLASES_DIR, DATA_DIR, SNYCQ_CLUSTERS_INFO_PATH\n",
    "from utils.basics import FB_400ROI_ATLAS_NAME, FB_400ROI_ATLAS_PATH, FB_400ROI_BRAINNET_NODES_PATH\n",
    "from utils.basics import get_sbj_scan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f685990-7066-4a0a-bd6b-be507ce5b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: RESOURCES_NBS_DIR=/data/SFIMJGC_Introspec/2023_fc_introspection/code/fc_introspection/resources/nbs\n"
     ]
    }
   ],
   "source": [
    "print(\"++ INFO: RESOURCES_NBS_DIR=%s\" % RESOURCES_NBS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a0615-b81e-47ad-87dc-f16cefdfcc3d",
   "metadata": {},
   "source": [
    "If the folder where we want to sace the outputs/inputs for NBS does not exists, we create it.\n",
    "\n",
    ">**NOTE:** If the folder already exists, it will delete its contents and you will have to re-run the NBS portion of the analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce74e1b2-bee1-4164-aca5-3483109e6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not osp.exists(RESOURCES_NBS_DIR):\n",
    "    os.makedirs(RESOURCES_NBS_DIR)\n",
    "    print('++ INFO: Created folder [%s]' % RESOURCES_NBS_DIR)\n",
    "for ATLAS_NAME in [FB_400ROI_ATLAS_NAME]:\n",
    "    folder_name = osp.join(RESOURCES_NBS_DIR,ATLAS_NAME)\n",
    "    if osp.exists(folder_name):\n",
    "        rmtree(folder_name)\n",
    "        print('++ WARNING: Removing pre-existing folder [%s]' % folder_name)\n",
    "    os.makedirs(folder_name)\n",
    "    print('++ INFO: Regenerating empty folder [%s]' % folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298f220-7c60-4fd8-8967-a5b78c913898",
   "metadata": {},
   "source": [
    "# 1. Create input files for NBS and BrainNet packages\n",
    "\n",
    "These two software requires a few additional files with information about ROI names, centroids and labels. We generate those next for the two atlases of interest:\n",
    "\n",
    "* ```<ATLAS_NAME>_BrainNet_Nodes.node```: Information about ROI names and centroids for BrainNet.\n",
    "* ```<ATLAS_NAME>_NBS_Node_Coordinates.txt```: ROI centroids in NBS format.\n",
    "* ```<ATLAS_NAME>_NBS_Node_Labels.txt```: ROI names in NBS format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9335ef0e-9bb7-45e7-ad7d-31d9f1a598da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nw2Id = {'Vis':1,'SomMot':2,'DorsAttn':3,'SalVentAttn':4,'Limbic':5,'Cont':6,'Default':7,'Subcortical':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee4c6ec-a847-48bb-9ce7-2a12aaa1e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: BrainNet_Node file written to disk:   [/data/SFIMJGC_Introspec/2023_fc_introspection/code/fc_introspection/resources/nbs/Schaefer2018_400Parcels_7Networks_AAL2/Schaefer2018_400Parcels_7Networks_AAL2_BrainNet_Nodes.node]\n",
      "++ INFO: NBS Coordinate file written to disk: [/data/SFIMJGC_Introspec/2023_fc_introspection/code/fc_introspection/resources/nbs/Schaefer2018_400Parcels_7Networks_AAL2/Schaefer2018_400Parcels_7Networks_AAL2_NBS_Node_Coordinates.txt]\n",
      "++ INFO: NBS Label file written to disk:      [/data/SFIMJGC_Introspec/2023_fc_introspection/code/fc_introspection/resources/nbs/Schaefer2018_400Parcels_7Networks_AAL2/Schaefer2018_400Parcels_7Networks_AAL2_NBS_Node_Labels.txt]\n",
      "++ =====================================\n"
     ]
    }
   ],
   "source": [
    "roi_info,Nrois={},{}\n",
    "for ATLAS_NAME,BRAINNET_NODES_PATH  in zip([FB_400ROI_ATLAS_NAME],[FB_400ROI_BRAINNET_NODES_PATH]):\n",
    "    # Load necessary info in memory\n",
    "    # =============================\n",
    "    ATLASINFO_PATH       = osp.join(ATLASES_DIR,ATLAS_NAME,'{ATLAS_NAME}.roi_info.csv'.format(ATLAS_NAME=ATLAS_NAME))\n",
    "    roi_info[ATLAS_NAME] = pd.read_csv(ATLASINFO_PATH)\n",
    "    Nrois[ATLAS_NAME]    = roi_info[ATLAS_NAME].shape[0]\n",
    "    # Create Brainnet Nodes data structure\n",
    "    # ====================================\n",
    "    BRAINNET_NODES_df               = roi_info[ATLAS_NAME][['pos_R','pos_A','pos_S','ROI_Name']].copy()\n",
    "    BRAINNET_NODES_df['Node Size']  = 1\n",
    "    BRAINNET_NODES_df['Node Color'] = [Nw2Id[n.split('_')[1]] for n in BRAINNET_NODES_df['ROI_Name']]\n",
    "    BRAINNET_NODES_df = BRAINNET_NODES_df[['pos_R','pos_A','pos_S','Node Color','Node Size','ROI_Name']]\n",
    "    # Save to disk\n",
    "    # ============\n",
    "    BRAINNET_NODES_df.to_csv(BRAINNET_NODES_PATH, sep=' ', index=None, header=None)\n",
    "    print('++ INFO: BrainNet_Node file written to disk:   [%s]' % BRAINNET_NODES_PATH)\n",
    "    # Save coordinate file to disk for NBS\n",
    "    # ====================================\n",
    "    coor_file_path = osp.join(RESOURCES_NBS_DIR,ATLAS_NAME,'{ATLAS_NAME}_NBS_Node_Coordinates.txt'.format(ATLAS_NAME=ATLAS_NAME))\n",
    "    BRAINNET_NODES_df[['pos_R','pos_A','pos_S']].to_csv(coor_file_path, sep=' ', index=None, header=None)\n",
    "    print(\"++ INFO: NBS Coordinate file written to disk: [%s]\" % coor_file_path)\n",
    "    # Save label file to disk for NBS\n",
    "    # ===============================\n",
    "    label_file_path = osp.join(RESOURCES_NBS_DIR,ATLAS_NAME,'{ATLAS_NAME}_NBS_Node_Labels.txt'.format(ATLAS_NAME=ATLAS_NAME))\n",
    "    BRAINNET_NODES_df['ROI_Name'].to_csv(label_file_path, sep=' ', index=None, header=None)\n",
    "    print(\"++ INFO: NBS Label file written to disk:      [%s]\" % label_file_path)\n",
    "    print(\"++ =====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f660070-0410-4fc3-9cb8-ea3201b0c164",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Create Design Matrix for NBS\n",
    "\n",
    "First, we load the cluster membership information generated in ```S12_SNYCQ_Clustering_Confounds```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171bd236-a713-4a1e-9fad-11e377c5941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_info = pd.read_csv(SNYCQ_CLUSTERS_INFO_PATH, index_col=['Subject','Run'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de255372-558b-4d09-a469-b64ea5ac787e",
   "metadata": {},
   "source": [
    "Second, we confirm that we have three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33ef496-139c-4435-b770-e998a51bb046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Number of Clusters = 3 clusters\n"
     ]
    }
   ],
   "source": [
    "N_clusters = len(clusters_info['Cluster ID'].unique())\n",
    "print(\"++ INFO: Number of Clusters = %d clusters\" % N_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5aa9bc-4c76-48d8-b616-5b549951c867",
   "metadata": {},
   "source": [
    "Third, we create a dictionary that will contain the scan_IDs for each of the three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c5b5c0-62ad-4dbc-9d64-bc6ef202db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scans_per_cluster={cl_label:clusters_info[clusters_info['Cluster Label']==cl_label].index for cl_label in ['Large F1','Large F2','Middle']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ddf73-d9c3-4b6e-9d5c-1dd482e772f5",
   "metadata": {},
   "source": [
    "Forth, we print again the number of scans per cluster, as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93cf73ce-6b67-48af-abd7-fbcea9019d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Large F1', (81,)), ('Large F2', (78,)), ('Middle', (312,))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k,i.shape) for k,i in scans_per_cluster.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da47a50-81a4-4493-9ee5-bdc229d55676",
   "metadata": {},
   "source": [
    "Fifth, we generate the design matrix taking into accoun only the scans that we use in this part of the analysis (namely those in clusters ```Large F1``` and ```Large F2```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cc6475-5cfe-4777-b61a-0f5aabcd063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Design Matrix for 3 Cluster solution saved in [/data/SFIMJGC_Introspec/2023_fc_introspection/code/fc_introspection/resources/nbs/NBS_CL02_DesingMatrix.txt]\n",
      "++ INFO: Design Matrix for 3 Cluster solution has shape [(159, 2)]\n"
     ]
    }
   ],
   "source": [
    "DESIGN_MATRIX = np.vstack([np.tile(np.array([1,0]),(len(scans_per_cluster['Large F1']),1)),\n",
    "                           np.tile(np.array([0,1]),(len(scans_per_cluster['Large F2']),1))])\n",
    "DESIGN_MATRIX_PATH = osp.join(RESOURCES_NBS_DIR,'NBS_CL02_DesingMatrix.txt')\n",
    "np.savetxt(DESIGN_MATRIX_PATH,DESIGN_MATRIX,delimiter=' ',fmt='%d')\n",
    "print('++ INFO: Design Matrix for 3 Cluster solution saved in [%s]' % DESIGN_MATRIX_PATH)\n",
    "print('++ INFO: Design Matrix for 3 Cluster solution has shape [%s]' % str(DESIGN_MATRIX.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84c887-9d81-4937-a0f5-790248484c16",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Create Copies of Scan-wise FC Matrices in NBS folders\n",
    "\n",
    "Count how many scans we have in total across the two clusters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01f4dec1-03c8-4464-9ed8-0d3ce2994b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_,scans_list = get_sbj_scan_list(when='post_motion', return_snycq=False)\n",
    "Nscans       = clusters_info.set_index('Cluster Label').loc[['Large F1','Large F2']].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d592338-07bf-4c1d-ac4b-8a6d27b066ee",
   "metadata": {},
   "source": [
    "## 3.1. Load all FC matrices into a XR.DataArray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1daa9d-1a3a-4c8e-834c-a13940c4bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.93 s, sys: 415 ms, total: 4.34 s\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sfc_Z_xr={}\n",
    "for ATLAS_NAME in [FB_400ROI_ATLAS_NAME]:\n",
    "    # Create Empty Numpy Array where to hold all FC matrices. At the end we will move this into an Xarray\n",
    "    # ===================================================================================================\n",
    "    sfc_Z_arr = np.empty((Nscans,Nrois[ATLAS_NAME],Nrois[ATLAS_NAME])) * np.nan\n",
    "    scan_idx      = 0\n",
    "    scan_name_idx = []\n",
    "    # For all clusters of interest\n",
    "    # ============================\n",
    "    for cluster_id in ['Large F1', 'Large F2']:\n",
    "        # For each scan in a given cluster\n",
    "        # ================================\n",
    "        for sbj,run in scans_per_cluster[cluster_id]:\n",
    "            # Load FC matrix from disk\n",
    "            # ========================\n",
    "            scan_name_idx.append('.'.join([sbj,run]))\n",
    "            _,_,sesID,_,runID,_,acqID = run.split('-')\n",
    "            sfc_path = osp.join(DATA_DIR,'PrcsData',sbj,'preprocessed','func','pb06_staticFC','{acqID}_run-{runID}.{ATLAS_NAME}_000.netcc'.format(acqID=acqID,runID=runID, ATLAS_NAME=ATLAS_NAME))\n",
    "            aux_cc_r = load_netcc(sfc_path)\n",
    "            # Apply Fisher's transformation\n",
    "            # =============================\n",
    "            aux_cc_Z = aux_cc_r.apply(np.arctanh)\n",
    "            np.fill_diagonal(aux_cc_Z.values,1)\n",
    "            sfc_Z_arr[scan_idx,:,:] = aux_cc_Z\n",
    "            # Update counter\n",
    "            # ==============\n",
    "            scan_idx = scan_idx + 1\n",
    "            del aux_cc_r, aux_cc_Z\n",
    "    # Save all FC matrixes for a given atlas in XR.Array Form\n",
    "    # =======================================================\n",
    "    sfc_Z_xr[ATLAS_NAME] = xr.DataArray(sfc_Z_arr,\n",
    "                        dims=['scan','roi_x','roi_y'],\n",
    "                        coords={'scan':scan_name_idx,\n",
    "                                'roi_x':roi_info[ATLAS_NAME]['ROI_ID'],\n",
    "                                'roi_y':roi_info[ATLAS_NAME]['ROI_ID']})\n",
    "    del sfc_Z_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac5b85-78d0-4e23-885c-36376da96ea7",
   "metadata": {},
   "source": [
    "## 3.2. Create Data Folder for this solution\n",
    "\n",
    "Once more, to make sure all things are consistent, we will create empty folders before we start copying the scan-wise FC matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "480d32f6-38b0-4695-9958-05092e025c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ATLAS_NAME in [FB_400ROI_ATLAS_NAME]:\n",
    "    NBS_CL02_matrices_folder = osp.join(RESOURCES_NBS_DIR,ATLAS_NAME,'NBS_CL02_Data')\n",
    "    print(\"++ WARNING: Removing pre-existing folder [%s]\" % NBS_CL02_matrices_folder)\n",
    "    if osp.exists(NBS_CL02_matrices_folder):\n",
    "        rmtree(NBS_CL02_matrices_folder)\n",
    "    os.mkdir(NBS_CL02_matrices_folder)\n",
    "    print(\"++ INFO: Creating empty folder [%s]\" % NBS_CL02_matrices_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d7f14-02b0-428a-85ce-f84f6d132afc",
   "metadata": {},
   "source": [
    "## 3.3. Make copies of matrices into the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "643dd2b1-3806-4e82-8f7e-f60995993975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atlas: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:19<00:00,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 383 ms, total: 17.5 s\n",
      "Wall time: 19.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ATLAS_NAME in tqdm([FB_400ROI_ATLAS_NAME], desc='Atlas'):\n",
    "    NBS_CL02_matrices_folder = osp.join(RESOURCES_NBS_DIR,ATLAS_NAME,'NBS_CL02_Data')\n",
    "    for i,item in enumerate(list(sfc_Z_xr[ATLAS_NAME].indexes['scan'])):\n",
    "        dest_path = osp.join(NBS_CL02_matrices_folder,'subject{id}.txt'.format(id=str(i+1).zfill(3)))\n",
    "        np.savetxt(dest_path,sfc_Z_xr[ATLAS_NAME].loc[item,:,:],delimiter=' ',fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "FC Introspection (Jan 2023)",
   "language": "python",
   "name": "fc_introspection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
